---
title: "Simple method to correlate infection spread and isolation"
author: "Adriano J. Holanda"
output: html_notebook
---

# Logistic function

```{r}
library(latex2exp)
library(ggplot2)
# Differential equations solver
library(deSolve)
df.logistic <- function (t, y, parms) {
  list(parms["r"] * y * (1-y/parms["K"]))
}
t_final <- 300
times <- seq(from=1, to=t_final, by=1)
y_initial <- c(y=1)
parms <- c(r=0.04, K=350)
out <- ode(func=df.logistic, y=y_initial, 
           times=times, parms=parms)
# Normalize (standardize) the data

for (i in 1:2) {
  out[,i] <- out[,i] / max(out[,i])
}
# Create the data frame from logistic output
df <- data.frame(t=out[,1], y=out[,2])
# Plot
pl_title <- "Logistic function"
pl <- ggplot(df, aes(x=t, y=y)) +
      geom_line(color="blue") +
      labs(title=pl_title, y="cases\n", x = "time") +
      annotate("text", x=.125, y=.9, 
               label=TeX("\\frac{dy}{dt} = r y (1 - y / K)"), 
               size=4)
pl
```
# Data

The data are processed in periods of time, we use days as 
unit for time and avoid the use of fractional values to 
simplify the code and calculations. Another justification 
for the discrete values for time is that we are handling 
approximations for the conditions studied.

 The value of `TIME_STEP` was adopted after a set of tests 
 were performed to check which value causes 
 a better correlation in the curves 
 for the model and estimated data. 
 This high correlation means that 
 the isolation level in that time step 
 affects the number of cases in the 
 next time step.

But if you want to test a different time step, 
here is the place to tweak:

```{r}
TIME_STEP <- 5 # days.
```

## COVID-19

The COVID-19 cases are gathered from 
["São Paulo Plan"](https://www.saopaulo.sp.gov.br/planosp/) 
 organized by the government of São Paulo State. Brazil 
 to concentrate efforts to handle the virus spread in the 
 state. The file containing the cases, and other data, 
 is downloaded from 
 [Github repository](https://github.com/seade-R/dados-covid-sp)
 for the initiative.

TODO: put an alternate source
```{r}
# Current directory
WD <- getwd();
# File with COVID-19 cases
fn <- "dados_covid_sp.csv"
# Download the data from original source
fp <- file.path(WD, fn);
if (file.exists(fp) == FALSE) {
  download.file(paste("https://raw.githubusercontent.com/seade-R/dados-covid-sp/master/data/", fn, sep = ""), fp)
}
# Read CSV file organizing the data according to columns
df <- read.csv(fp, header = TRUE, sep = ";");
# Sum the cases for all cities by date
dfc <- aggregate(casos_novos~datahora, FUN=sum, data=df)
# Rename the data frame columns
names(dfc)[names(dfc) == "datahora"] <- "date"
names(dfc)[names(dfc) == "casos_novos"] <- "cases"
```

## Isolation level

The isolation level for all cities in the São Paulo State are 
stored in an Excel file downloaded from 
[``São Paulo Government Isolation Monitoring'' site](https://www.saopaulo.sp.gov.br/coronavirus/isolamento/).
 The file must be downloaded manually after clicking in the ``DADOS'' button, 
 then click ``Baixar'' icon, choose ``Tabela de referência cruzada',' ``Excel'' format
  and click ``Baixar'' button. 
 We choose the Excel format because was the content that could be parsed with 
 reliability. The dates are synchronized with the ones in cases data, only the 
 format is different ("DD/MM/YY"), but this is fixed by changing it to
 "YYYY-MM-DD".

```{r}
library(readxl)
fn <- "Dados.xlsx"
fp <- file.path(WD, fn)
df <- read_excel(fp)
# The isolation level data start at 5th column and the
# first row is the date, the rest are isolation level 
# values.
shift <- 4 # 5th column because indices in R start at 1
dates <- vector("character", ncol(df) - shift)
values <- vector("numeric", ncol(df) - shift)
for (j in (shift+1):ncol(df)) {
  # process dates
  tmp <- toString(df[1,j])
  tmp <- unlist(strsplit(tmp, "/"))
  # convert "DD/MM/YY" to "YYYY-MM-DD"
  dates[j-shift] <- paste(paste("20", tmp[[3]], sep=""), tmp[[2]], tmp[[1]], sep="-")
  # process isolation level values
  tmps <- as.numeric(unlist(as.list(df[2:nrow(df),j])))
  good <- complete.cases(tmps)
  values[j-shift] <- mean(tmps[good])
}
dfl <- data.frame(date=dates, level=values)
```

## Processing

The sum of cases in a time step (period) are accumulated 
and the last date of the period is associated with the
cumulative value. The same reasoning for isolation level, 
with the exception that the levels are averaged in the period.

The isolation level data frame `dfl` is used to calculate
the number of steps `T_TIME_STEPS` because its update
is delayed when compared with the case values update.

```{r}
# Cases are accumulated along the time
IS.CUMULATIVE <- TRUE
# Number of elements in the data frame
N_TIME_STEPS <- floor(length(dfl[,1]) /  TIME_STEP)
cases_dt <- vector("numeric", N_TIME_STEPS)
levels_dt <- vector("numeric", N_TIME_STEPS)
dates_dt <- vector("character", N_TIME_STEPS)
# Find the intersection of dates to avoid mismatch.
dates <- intersect(dfc[,"date"], dfl[,"date"])
count_cases <- 0
count_level <- 0.0
count <- 0
for (date in dates) {
  count <- count + 1
  count_cases <- count_cases + subset(dfc, date==date)[2]
  count_level <- count_level + subset(dfl, date==date)[2]
  
  if ((count %% TIME_STEP) == 0) {
    k <- count / TIME_STEP
    print(k)
    dates_dt[k]  <- date
    values_dt[k] <- count_cases
    levels_dt[k] <- count_level / TIME_STEP
    count_level <- 0.0
    if (IS.CUMULATIVE == FALSE) {
      count_cases <- 0
    }
  }
}
#dfdt = data.frame(date=dates_dt, empirical=cases_dt, level=levels_dt)
```

```{r}
# Vectors to store the number of cases and average 
# of isolation levels at each time step
empirical <- vector("numeric", N_TIME_STEPS)
level <- vector("numeric", N_TIME_STEPS)
for (i in 1:N_TIME_STEPS) {
  k <- i*W
  # Assign the sum the cases from rows [1,k] to ith time step
  empirical[i] = sum(df[1:k, 2])
  # Assign the sum the cases from rows [j,k] to ith time step
  j = (i-1)*W + 1
  level[i] = mean(df[j:k, 3])
}
dfdt <- data.frame(level, empirical)
# Add time steps to data frame
time <- seq(from=1, to=length(dfdt$level), by=1)
dfdt$time <- time
```

# Fitting

Logistic function in the exponential form:

$$𝑓(t)= {K \over 1 + ({ K-m \over m }) e^{−rt}}$$


where:
t is a list of values representing the time steps;
n_m is the sigmoid's midpoint;
K is the the maximum value for n;
r is the the logistic growth rate.

The logistic function is used to fit the number of cases and the 
fitting parameters are used in the estimation phase.


```{r}
library(ggplot2)
library(minpack.lm)
temp <- data.frame(y = dfdt$empirical, x = seq(length(dfdt$empirical)))
logistic.model <- nlsLM(y ~ (K/ (1 + ((K-m)/m)*exp(- r * x))), 
                        data = temp, 
                        start = list(K = 1, m=1, r = 0.1))
summary(logistic.model)
fitting <- predict(logistic.model, list(x = temp$x))
dfdt$fitting <- fitting

# Plot
p_title <- paste("Fitting of empirical data using", W, "days as time step")
colors <- c("empirical" = "red", "fitted" = "blue")
p <- ggplot(dfdt, aes(x=time, y=empirical)) + 
     geom_point(color = "red") +
     geom_line(aes(x=time, y=fitting, color = "blue"), size=1) +
     labs(title=p_title, y="cases\n", x = "time") +
     scale_color_identity(name = "",
                          breaks = c("blue"),
                          labels = c("fit"),
                          guide = "legend")
p

# Test the correlation of the empirical data and the model
cor.test(temp$y, dfdt$fitting, method=c("pearson"))

```
# Estimating

The estimation is performed after the calculation of $\lambda$
 is done by:
 
 $$\lambda = r_{fit}\, \overline{i\ },$$
where $r_{fit}$ is the logistic growth factor obtained from 
fitting and $\overline{i }$ is the average of all isolation 
level values.

The $\lambda$ value is used to estimate the number of cases 
$n(t+dt)$ using the logistic function in the exponential form, 
substituting the parameters by the fitted ones with the exception 
of growth factor from previous time step that now depends on 
isolation level of that step and it's calculated by

$$r(t) = {\lambda\over i(t)}, $$
and the logistic function calculates the current number of cases based 
on the previous time step isolation level using:

$$n(t+dt)= {K \over 1 + ({ K-n_m \over n_m }) e^{−r(t)\, t}}$$

```{r}
# Estimation of cases using fitted parameters and 
# the mean of isolation level at each time step.
exp.logistic <- function(t, r_t, K, m) {
    y <- K / (1 + ((K-m)/m)*exp(-r_t * t))
    y
}

# Calculate the constant alpha
r_fit <- coef(logistic.model)["r"]
lamb <- r_fit * mean(dfdt$level)

K <- coef(logistic.model)["K"]
m <- coef(logistic.model)["m"]
# Estimate the number of cases N(t+dt)
estimated = vector("numeric", NW)
# Estimate the first case manually, but not the rest.
estimated[1] = 1
for (t in 2:NW) {
  r_t <- lamb/dfdt$level[t]
  # This is t+dt compared with empirical time
  estimated[t] <- exp.logistic(t, r_t, K, m)
}
dfdt$estimated <- estimated

# subplot in 2 rows
library(ggplot2)
library(reshape2)
p1_title = paste("Estimation of cases using", W, "days as time step")
p1_label = c("empirical", "estimated")
# melt the data to a long format
temp <- data.frame(time=dfdt$time, empirical=dfdt$empirical, estimated=dfdt$estimated)
df2dt <- melt(data = temp, id.vars = "time")
p1 <- ggplot(data = df2dt, aes(x = time, y = value, color = variable)) + 
      geom_point() + 
      labs(title=p1_title, y="cases\n", x = "") +
      scale_colour_discrete(name = "data type", labels = c("empirical", "estimated")) +
      theme(axis.title.x = element_blank(), axis.text.x = element_blank())

# plot 2
p2 <- ggplot(data = dfdt, aes(x = time, y = level, color = "")) + 
      geom_point() + 
      labs(y="level\n", x = "time") +
      scale_colour_discrete(name = "isolation", labels = c(""))
      
library(grid)
grid.newpage()
grid.draw(rbind(ggplotGrob(p1), ggplotGrob(p2), size = "last"))

# Test the correlation of the empirical and estimated data.
cor.test(dfdt$empirical, dfdt$estimated, method=c("pearson"))
```
