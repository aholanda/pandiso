---
title: "Simple method to correlate infection spread and isolation using logistic function"
author: "Adriano J. Holanda"
output: html_notebook
---

# Introduction

The logistic function is used to estimate the number 
of individuals infected by a virus in a time $t+1$
using the average isolation level in a population at a time $t$. 
Each time step is composed by a date range in days 
where the isolation level is supposed to take effect over the 
virus spread.

# Logistic function

```{r}
library(latex2exp)
# Smooth the line plotted
require(splines)
# Differential equations solver
library(deSolve)
df.logistic <- function (t, y, parms) {
  list(parms["r"] * y * (1-y/parms["K"]))
}
t_final <- 300
times <- seq(from=1, to=t_final, by=1)
y_initial <- c(y=1)
parms <- c(r=0.04, K=350)
out <- ode(func=df.logistic, y=y_initial, 
           times=times, parms=parms)
# Normalize (standardize) the data

for (i in 1:2) {
  out[,i] <- out[,i] / max(out[,i])
}
# Create the data frame from logistic output
df <- data.frame(t=out[,1], y=out[,2])
# Plot
xy <- predict(interpSpline(df$t, df$y))
plot(xy,
     type = "l",
     main = "Logistic function",
     xlab = "t",
     ylab = "y")

#annotate("text", x=.125, y=.9, 
#               label=TeX("\\frac{dy}{dt} = r y (1 - y / K)"), 
#               size=4)
rm(df, i, out, parms, times, t_final, xy, y_initial)
```
# Phase 0 - Data Organization

The data are processed in periods of time, we use days as 
unit for time and avoid the use of fractional values to 
simplify the code and calculations. Another justification 
for the discrete values for time is that we are handling 
approximations for the conditions studied.

 The value of `DATE_STEP` was adopted after a set of tests 
 were performed to check which value causes 
 a better correlation in the curves 
 for the model and estimated data. 
 This high correlation means that 
 the isolation level in that time step 
 affects the number of cases in the 
 next time step.

But if you want to test a different time step, 
here is the place to tweak:

```{r}
DATE_STEP <- 5 # days.
```

## COVID-19 cases

The COVID-19 cases are gathered from 
["São Paulo Plan"](https://www.saopaulo.sp.gov.br/planosp/) 
 organized by the government of São Paulo State. Brazil 
 to concentrate efforts to handle the virus spread in the 
 state. The file containing the cases, and other data, 
 is downloaded from 
 [Github repository](https://github.com/seade-R/dados-covid-sp)
 for the initiative.

TODO: put an alternate source
```{r}
# Current directory
WD <- getwd();
# File with COVID-19 cases
fn <- "dados_covid_sp.csv"
# Download the data from original source
fp <- file.path(WD, fn);
if (file.exists(fp) == FALSE) {
  download.file(paste("https://raw.githubusercontent.com/seade-R/dados-covid-sp/master/data/", fn, sep = ""), fp)
}
# Read CSV file organizing the data according to columns
df <- read.csv(fp, header = TRUE, sep = ";");
# Sum the cases for all cities by date
dfc <- aggregate(casos_novos~datahora, FUN=sum, data=df)
# Rename the data frame columns
names(dfc)[names(dfc) == "datahora"] <- "date"
names(dfc)[names(dfc) == "casos_novos"] <- "cases"
# Convert date string to date Class
dfc$date <- as.Date(dfc$date)
rm(fn)
```

## Population isolation level

The isolation level for all cities in the São Paulo State are 
stored in an Excel file downloaded from 
[``São Paulo Government Isolation Monitoring'' site](https://www.saopaulo.sp.gov.br/coronavirus/isolamento/).

The file must be downloaded manually after clicking in the "DADOS" button, 
then click "Baixar" icon, choose "Tabela de referência cruzada", "Excel" format
  and click "Baixar" button. 
 We choose the Excel format because its content could be parsed with 
 reliability. The isolation dates are synchronized cases data, only the 
 format is different ("DD/MM/YY"), but this is fixed by changing it to
 "YYYY-MM-DD".

```{r}
library(readxl)
fn <- "Dados.xlsx"
fp <- file.path(WD, fn)
df <- read_excel(fp)
# The isolation level data start at 5th column and the
# first row is the date, the rest are isolation level 
# values.
shift <- 4 # 5th column because indices in R start at 1
dates <- vector("character", ncol(df) - shift)
values <- vector("numeric", ncol(df) - shift)
for (j in (shift+1):ncol(df)) {
  # process dates
  tmp <- toString(df[1,j])
  tmp <- unlist(strsplit(tmp, "/"))
  # convert "DD/MM/YY" to "YYYY-MM-DD"
  tmp <- paste(paste("20", tmp[[3]], sep=""), tmp[[2]], tmp[[1]], sep="-")
  dates[j-shift] <- as.character(tmp)
  # process isolation level values
  tmps <- as.numeric(unlist(as.list(df[2:nrow(df),j])))
  # mark non-empty cells, in some cells the value is missing
  good <- complete.cases(tmps)
  values[j-shift] <- mean(tmps[good])
}
dfl <- data.frame(date=as.Date(dates), level=values)
rm(fn, good, j, shift, tmp, tmps, values)
```

## Processing

The sum of cases in a time step (period) are accumulated 
and the last date of the period is associated with the
cumulative value. The same reasoning for isolation level, 
with the exception that the levels are averaged in the period.

The isolation level data frame `dfl` is used to calculate
the number of steps `N_TIME_STEPS` because its update
is delayed when compared with the case values update.

```{r}
# Cases are accumulated along the time
IS.CUMULATIVE <- TRUE
# Number of elements in the data frame
N_TIME_STEPS <- floor(length(dfl[,1]) /  DATE_STEP)
cases_dt <- vector("numeric", N_TIME_STEPS)
levels_dt <- vector("numeric", N_TIME_STEPS)
dates_dt <- vector("character", N_TIME_STEPS)
# Find the intersection of dates to avoid mismatch.
dates <- intersect(as.character(dfc[,"date"]), as.character(dfl[,"date"]))
for (i in 1:N_TIME_STEPS) {
  # Time step [inf, sup]
  # time lower bound
  inf <- DATE_STEP * (i-1) + 1
  # time upper bound
  sup <- DATE_STEP*i
  
  # Date interval is marked by the upper bound
  dates_dt[i] <- dates[sup]
  # The levels are averaged
  levels_dt[i] <- mean(dfl[dfl$date >= dates[inf] & dfl$date <= dates[sup],]$level)
  
  if (IS.CUMULATIVE == TRUE) {
    inf <- 1
  }
  # The cases are summed since from the first case if IS.CUMULATIVE is TRUE
  cases_dt[i] <-  sum(dfc[dfc$date >= dates[inf] & dfc$date <= dates[sup],]$cases)
}
dfdt = data.frame(date=dates_dt, empirical=cases_dt, level=levels_dt)
# Add time steps to data frame
times <- seq(from=1, to=length(dfdt$level), by=1)
dfdt$time <- times
# Clean up some variables not needed anymore
rm(cases_dt, dates, dates_dt, dfl, dfc, i, levels_dt, times)
```

# Phase 1 - Fitting

Logistic function in the exponential form:

$$𝑓(t)= {K \over 1 + ({ K-m \over m }) e^{−rt}}$$


where:
t is a list of values representing the time steps;
n_m is the sigmoid's midpoint;
K is the the maximum value for n;
r is the the logistic growth rate.

The logistic function is used to fit the number of cases and the 
fitting parameters are used in the estimation phase.


```{r}
library(minpack.lm)
temp <- data.frame(y = dfdt$empirical, x = seq(length(dfdt$empirical)))
logistic.model <- nlsLM(y ~ (K/ (1 + ((K-m)/m)*exp(- r * x))), 
                        data = temp, 
                        start = list(K = 1, m=1, r = 0.1))
summary(logistic.model)
fitting <- predict(logistic.model, list(x = temp$x))
dfdt$fitting <- fitting
rm(fitting)

# Plot
plot(temp$x, temp$y, xlab = "time", ylab = "cases", 
     main = paste("Fitting of empirical data using", DATE_STEP, "days as date step"))
logistic.model <- nlsLM(y ~ (k / (1 + ((k-m)/m)*exp(- r * x))), data = temp, start = list(k = 1, m=1, r = 0.1))
ys_model <- predict(logistic.model, list(x = temp$x))
lines(temp$x, ys_model)

# TODO: review plot title
# TODO: rethink correlation method

# Test the correlation of the empirical data and the model
cor.test(temp$y, dfdt$fitting, method=c("pearson"))
rm(ys_model)
```
# Phase 2 - Estimating

The estimation is performed after the calculation of $\lambda$
 is done by:
 
 $$\lambda = r_{fit}\, \overline{i\ },$$
 
\noindent where $r_{fit}$ is the logistic growth factor obtained from 
fitting and $\overline{i }$ is the average of all isolation 
level values.

The $\lambda$ value is used to estimate the number of cases 
$n(t+dt)$ using the logistic function in the exponential form, 
substituting the parameters by the fitted ones with the exception 
of growth factor from previous time step that now depends on 
isolation level of that step and it's calculated by

$$r(t) = {\lambda\over i(t)}, $$
and the logistic function calculates the current number of cases based 
on the previous time step isolation level using:

$$n(t+dt)= {K \over 1 + ({ K-n_m \over n_m }) e^{−r(t)\, t}}$$

```{r}
library(emdbook)  # scinot
# Estimation of cases using fitted parameters and 
# the mean of isolation level at each time step.
exp.logistic <- function(t, r_t, K, m) {
    y <- K / (1 + ((K-m)/m)*exp(-r_t * t))
    y
}

# Calculate the constant alpha
r_fit <- coef(logistic.model)["r"]
lamb <- r_fit * mean(dfdt$level)

K <- coef(logistic.model)["k"]
m <- coef(logistic.model)["m"]
# Estimate the number of cases N(t+dt)
estimated = vector("numeric", N_TIME_STEPS)
# Estimate the first case manually, but not the rest.
estimated[1] = 1
for (i in 2:N_TIME_STEPS) {
  r_t <- lamb/dfdt$level[i]
  # This is t+dt compared with empirical time
  estimated[i] <- exp.logistic(i, r_t, K, m)
}
dfdt$estimated <- estimated
rm(estimated)

# Subplot in 2 rows, same x
par(mfrow = c(2, 1), mar = c(0, 4.1, 4, 2))
## Subplot 1
plot(dfdt$time, dfdt$empirical, frame.plot = TRUE,
     axes = FALSE, yaxt = "none",
     pch = 19, col = 1, xlab = "", ylab = "cases")
#### add y axis
axis.scinot(side=2, c(2*10^6, 4*10^6))  # TODO: plug with y values
### plot another set of points
points(dfdt$time, dfdt$estimated, pch = 19, col = "gray49")
### add legend
legend("topleft", legend = c(names(dfdt)[2], names(dfdt)[6]), 
       col = c(1, "gray49"), pch = 19, bty = "n")
title(main = paste("Date step=", DATE_STEP, " days", sep = ""))
mtext("a)", side = 3, at = -17)

## Subplot 2
par(mar = c(4, 4.1, 0, 2))
plot(dfdt$time, dfdt$level, pch = 19, col = "gray35", 
     axes = FALSE, frame.plot = TRUE,
     xlab = names(dfdt)[4], ylab = names(dfdt)[3])
### add legend
legend("topright", legend = "isolation", 
       col = "gray35", pch = 19, bty = "y")
#### add x axis
axis(side = 1)
#### add y axis
axis(side = 2, las = 1)
### add subplot 2 identifier
mtext("b)", side = 3, at = -17)
mtext(paste("time span=[", dfdt$date[1], ", ",dfdt$date[length(dfdt$date)], "]", sep = ""), 
      side = 3, line = -1, cex=.75)

# Test the correlation of the empirical and estimated data.
cor.test(dfdt$empirical, dfdt$estimated, method=c("pearson"))
rm(df)
```
